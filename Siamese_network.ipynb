{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014a4d74-28a3-4d8a-94c1-906b26b5d558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/faceidenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903506e-d204-40a6-beed-334e51578b9b",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6521d5f0-70ce-4408-aef2-80f6a5db6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np \n",
    "import random \n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Layer, Input\n",
    "import tensorflow as tf\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca47bd0-7129-4ee6-b09b-6da7f245196c",
   "metadata": {},
   "source": [
    "### Creating the Image paths\n",
    "It is time to create folders to store anchor, positive and negative images. First, the folder path's are stored in the corresponding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d86cc-a67e-4088-897e-7e40f9b52155",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebff5f0-b4a0-4a0a-9600-90a132d8a953",
   "metadata": {},
   "source": [
    "Now we create the directories using the given folder paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1ea9d2-ada1-4a92-8ba7-f3edbd159b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(POS_PATH)\n",
    "#os.makedirs(NEG_PATH)\n",
    "#os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb584b4-8f0a-4711-a55f-e8d15a00359e",
   "metadata": {},
   "source": [
    "The negative images have been collected from the Labelled Faces in the Wild (LFW) dataset. This dataset consists of over 13000 labelled faces collected from the web. Now, it is time to shift these images from their respective directories to the Negative directory that has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec292e09-8cb7-4298-b373-2e5b33894390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw'):\n",
    "    if directory == '.DS_Store':\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        ex_path = os.path.join('lfw', directory, file)\n",
    "        new_path = os.path.join(NEG_PATH, file)\n",
    "        os.replace(ex_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25beaffc-9167-45a8-a218-a7116acb945c",
   "metadata": {},
   "source": [
    "## Collecting Anchor and Positive Images\n",
    "Now, since the negative images have already been collected from the LFW dataset, the anchor and positive images are collected manually using the cv2 library. The anchor images and positive images are stored in the anchor and positive directories respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8dd692-043c-45fd-a111-92b21b5d22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[450:900, 750:1200, :]\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        img_name = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        img_name = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(img_name, frame)        \n",
    "    \n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5549f4-7da8-4322-914d-9be31d84c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfd309-7eb5-4d80-ab0b-c84a206389a1",
   "metadata": {},
   "source": [
    "### Storing file paths into a variable\n",
    "\n",
    "In order to now build the model, 300 images from the anchor, positive and negative directories are selected at random. These are stored in separate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0154775a-1c35-4e51-a9ef-f646cb268ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(ANC_PATH + '/*.jpg').take(300)\n",
    "positive= tf.data.Dataset.list_files(POS_PATH + '/*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH + '/*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6bba7-0d32-497a-9027-ebd496ff0d3b",
   "metadata": {},
   "source": [
    "### Preprocessing function\n",
    "The anchor, positive and negative variables currently only contain the file paths to the individual images. However, in order to visualise the images, a preprocess function is constructed which takes the file path and returns the image in a normalized tensor format suitable for model input.\n",
    "\n",
    "The function performs several key steps:\n",
    "\n",
    "1. **File Reading**: Uses `tf.io.read_file()` to read the image file from the given path as raw bytes\n",
    "2. **Image Decoding**: Converts the byte data into a tensor using `tf.io.decode_jpeg()` \n",
    "3. **Resizing**: Standardizes all images to 100x100 pixels using `tf.image.resize()` to ensure consistent input dimensions\n",
    "4. **Normalization**: Divides pixel values by 255 to scale them from the range [0,255] to [0,1], which helps with model training stability and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0eaf43-d577-4570-92c9-c8cb452d1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path): \n",
    "    byte_img = tf.io.read_file(file_path) ## reads file  \n",
    "    img = tf.io.decode_jpeg(byte_img)  ## decodes the bytes into tensor\n",
    "    img = tf.image.resize(img, (100,100)) ##resizing the image into (100,100)\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db51499-c7ea-491a-9d3c-2a597232df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "img = preprocess('data/anchor/07a799fa-679f-11f0-8ad3-b2f4ed02152c.jpg')\n",
    "#plt.imshow(img)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd20b81-fbe5-4c43-a3c4-049f4812bf12",
   "metadata": {},
   "source": [
    "### Constructing the dataset\n",
    "Now in order to create the actual dataset, a dataset containing both positive and negative examples is created. The dataset contains both positive pairs (same person) and negative pairs (different people). This is done by adding label 1 to each positive pair and label 0 for each negative pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c38f934f-a037-4140-b09f-e91b1f7b7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip(anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))))\n",
    "negatives = tf.data.Dataset.zip(anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15b38115-72b7-4b11-a2c2-337fcbc1ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()\n",
    "example = samples.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c24227-2c53-42a2-bea3-9d4d8f99e799",
   "metadata": {},
   "source": [
    "Another function is created which takes as input the input image, validation image and the corresponding label adn then returns all of them preprocessed (using the function defined earlier. This is because since the data object created only contains the file paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df091655-64ed-42fa-a812-ba85ec35164f",
   "metadata": {},
   "source": [
    "### Partioning into Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b620243-b026-43cc-955d-5cb324aae472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f48c5abc-33c7-456f-a35b-77e66dc6a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee0662d-8178-410d-813a-a958f1c47fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.take(round(len(data) *.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4be7a6-9f45-4277-95bd-c01c8f4c7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.skip(round(len(data) * .7))\n",
    "test_data = data.take(round(len(data) *.3))\n",
    "test_data = test_data.batch(16) \n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0c34d-e1af-4572-be99-8f9b15c939aa",
   "metadata": {},
   "source": [
    "### Creating the CNN backbone\n",
    "\n",
    "The embedding layer takes images as input and converts them into feature vectors (embeddings). The model learns to create similar embeddings for images of the same person and different embeddings for images of different people. This allows the network to effectively compare faces by measuring the L1 distance between their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c3e066b-1e30-4326-b4f9-0d957c825e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape = (100,100,3), name = 'input_image')\n",
    "    c1 = Conv2D(filters = 64, kernel_size = (10,10), activation = 'relu')(inp)\n",
    "    p1 = MaxPooling2D(64, (2,2), padding = 'same')(c1)\n",
    "    c2 = Conv2D(filters = 128, kernel_size = (7,7), activation = 'relu')(p1)\n",
    "    p2 = MaxPooling2D(64, (2,2), padding = 'same')(c2)\n",
    "    c3 = Conv2D(filters = 128, kernel_size = (4,4), activation = 'relu')(p2)\n",
    "    p3 = MaxPooling2D(64,(2,2), padding = 'same')(c3)\n",
    "    c4 = Conv2D(filters = 256, kernel_size = (4,4),activation = 'relu')(p3) \n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation = 'sigmoid')(f1)\n",
    "    return Model(inputs = inp, outputs = d1, name = 'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b0988db9-b9aa-4479-b4be-ad63879df1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"embedding\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"embedding\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">37,752,832</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_image (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m91\u001b[0m, \u001b[38;5;34m91\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m19,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m401,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m37,752,832\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,960,448</span> (148.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,960,448\u001b[0m (148.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,960,448</span> (148.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,960,448\u001b[0m (148.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = make_embedding()\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3cd87431-9a4a-41d2-b57d-32c5462ce40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee5214be-4d0a-4a62-adf0-e1b24ef80c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    input_img = Input(shape = (100,100,3), name = 'input_image')\n",
    "    validation_img = Input(shape = (100,100,3), name = 'validation_image')\n",
    "\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distances'\n",
    "    distances = siamese_layer(embedding(input_img), embedding(validation_img))\n",
    "    classifier = Dense(1, activation = 'sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs = [input_img, validation_img], outputs = classifier, name = 'Siamese-model')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eedc91cf-823f-4957-b404-8c3a8f73bb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Siamese-model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Siamese-model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_image         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ validation_image    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,960,448</span> │ input_image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ validation_image… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_dist_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L1Dist</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,097</span> │ l1_dist_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_image         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ validation_image    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)      │ \u001b[38;5;34m38,960,448\u001b[0m │ input_image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ validation_image… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_dist_5 (\u001b[38;5;33mL1Dist\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m4\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ embedding[\u001b[38;5;34m5\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m4,097\u001b[0m │ l1_dist_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,964,545</span> (148.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,964,545\u001b[0m (148.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,964,545</span> (148.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,964,545\u001b[0m (148.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f862b5e-7190-48bc-88fe-0f2c8635bf82",
   "metadata": {},
   "source": [
    "### Defining the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93c80614-638c-4f64-afe2-c330368dfc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16517909-dde1-4518-a173-4f35f793bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer = optimizer, siamese_model = siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456c466-2d2f-43b6-89a6-efad7b34a19f",
   "metadata": {},
   "source": [
    "### Creating custom training loop using tf.GradientTape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3c6a946-531b-46c7-83af-e0fad4ddd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "\n",
    "        yhat = siamese_model(X, training = True)\n",
    "        loss = loss_function(y,yhat)\n",
    "\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "712e11f1-f8a3-40be-bab7-79d023f6dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,epochs):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print((\"Epoch {}/{}\").format(epoch, epochs))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "\n",
    "        for idx, batch in enumerate(data):\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78a8f2a5-c314-4259-a67b-8e7a00b1cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a6f4793-7a7b-4678-a999-f3002564eecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:49:07.888771: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:50:30.239363: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:51:53.009460: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:53:15.911269: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:54:36.516658: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:55:59.052321: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:57:18.611959: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:58:37.233755: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 23:59:58.644868: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:01:20.852327: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:02:43.364437: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:04:05.396926: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:05:28.532132: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:06:51.418341: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:08:13.326014: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:09:34.761210: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:10:57.364945: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:12:17.346943: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 3s/step\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:13:35.776007: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:14:55.658092: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:16:17.073567: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:17:38.426528: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:19:02.018990: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:20:23.813140: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:21:43.946678: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:23:05.808546: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:24:27.581778: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:25:53.081120: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:27:15.873463: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:28:34.549158: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:29:56.978782: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:31:18.586107: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:32:39.555861: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:34:01.803251: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:35:26.988735: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:36:51.030456: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:38:11.831726: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:39:33.215580: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:40:57.848306: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:42:19.188220: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:43:40.616377: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:45:01.580035: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:46:21.444002: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:47:41.220155: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:49:01.570705: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:50:22.838443: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:51:42.927747: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:53:03.955399: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:54:26.402864: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 00:55:46.903827: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e2d74-9eef-480d-ae9a-8aff6aa5cc81",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "In order to see if our model performs well, the precision and recall of the model's predictions are tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4e6432e-bf94-447d-817e-6c357d1a5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e8fba16-b39e-40ff-a76a-e49dabbcddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f92fd43-6f5c-43c2-af57-c2f8b9e650ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7867adc9-ab1c-4889-a2f9-834ab72e0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.6781151e-11],\n",
       "       [1.0000000e+00],\n",
       "       [3.6550118e-10],\n",
       "       [9.9999809e-01],\n",
       "       [1.2250108e-11],\n",
       "       [1.0000000e+00],\n",
       "       [2.3620935e-08],\n",
       "       [9.9997634e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.7891752e-11],\n",
       "       [1.0000000e+00],\n",
       "       [5.9903812e-08],\n",
       "       [9.9993932e-01],\n",
       "       [9.9999750e-01],\n",
       "       [5.4349214e-10],\n",
       "       [5.0861200e-07]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = siamese_model.predict([test_inputs, test_val])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df8eb291-3720-4ab8-9fb8-472de04e3ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = [1 if pred > 0.5 else 0 for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d2cd2b9-b4c5-4315-9a86-b22cd7803dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = Recall()\n",
    "recall.update_state(y_true, y_hat)\n",
    "recall.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c713d1f6-140c-41ef-9e81-7dd080d79085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = Precision()\n",
    "precision.update_state(y_true,y_hat)\n",
    "precision.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771985c-1727-4f34-80ce-65938e19ea2b",
   "metadata": {},
   "source": [
    "Recall and Precision both being 1 is a good sign even though these results are only for a singular batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ca51ca-e92a-4efd-940e-c4e841c88aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (16,8))\n",
    "\n",
    "#plt.subplot(1,2,1)\n",
    "#plt.imshow(test_inputs[3])\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "#plt.imshow(test_val[3])\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe7b4dc-0e8a-4789-9919-32f8933a113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#siamese_model.save('siamese_model.keras')\n",
    "#model = tf.keras.models.load_model('siamese_model.keras', custom_objects = {'L1Dist': L1Dist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3516f08-d258-4704-88df-2d03b26f6314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.6874005e-07],\n",
       "       [1.0000000e+00],\n",
       "       [3.3164984e-08],\n",
       "       [9.9990344e-01],\n",
       "       [9.6981407e-12],\n",
       "       [1.0000000e+00],\n",
       "       [5.1810750e-10],\n",
       "       [9.9999982e-01],\n",
       "       [1.0000000e+00],\n",
       "       [5.6202262e-11],\n",
       "       [1.0000000e+00],\n",
       "       [4.9784902e-04],\n",
       "       [9.9992746e-01],\n",
       "       [1.0000000e+00],\n",
       "       [2.1414872e-09],\n",
       "       [9.4013084e-03]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([test_inputs, test_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cb880-afe3-4135-a7ff-610bf2e36aed",
   "metadata": {},
   "source": [
    "### Building the verification function \n",
    "\n",
    "This function handles the real-time verification process by comparing a captured input image against all of the stored verification images. It takes the current frame, the trained model, and two threshold parameters as inputs.\n",
    "\n",
    "The function iterates through each verification image, preprocessing both the input and verification images to the required format. The model then predicts the similarity between each pair, returning a value between 0 and 1. \n",
    "\n",
    "A detection count is calculated by counting how many predictions exceed the detection threshold. The overall verification score is determined by dividing successful detections by the total number of verification images. Finally, the person is considered verified if this score exceeds the verification threshold, providing a robust verification system that requires multiple positive matches rather than relying on a single comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d889c54-afcb-494a-ad98-e632f33d150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(frame, model, verification_threshold, detection_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_image')):\n",
    "        input_image = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        verification_image = preprocess(os.path.join('application_data', 'verification_image', image))\n",
    "\n",
    "        result = model.predict(list(np.expand_dims([input_image, verification_image], axis = 1)), verbose = 0)\n",
    "        results.append(result)\n",
    "\n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_image')))\n",
    "\n",
    "    verified = verification > verification_threshold\n",
    "\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28a626a3-3d2a-4bf5-a5f3-f844f61d4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#checkpoint_path = os.path.join('application_data', 'verification_image', '.ipynb_checkpoints')\n",
    "#if os.path.exists(checkpoint_path):\n",
    "    #shutil.rmtree(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7fcd3c43-eec1-427e-a713-208e3114a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[450:900, 750:1200, :]\n",
    "\n",
    "    cv2.imshow('Image verification', frame)\n",
    "    \n",
    "    key = cv2.waitKey(10) & 0xFF \n",
    "    if key == ord('v'): \n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "        results, verified = verify(frame, model, 0.5, 0.7)\n",
    "        print(verified)\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
